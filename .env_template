# file path and directory path configuration
CPA_FILE_PATH = "/home/azureuser/cpa-lrm-evaluation/doc/CPA_Chinese.xlsx"
RESULT_OUTPUT_DIR_PATH = "/home/azureuser/cpa-lrm-evaluation/output"

# deep seek have the below options.
# azure DeepSeek configuration has two options(#1 use the azure serverless deepseek R1, #2 use the github deepseek R1)ï¼Œthey can be called by Azure inference sdk
AZURE_INFERENCE_ROUND_ROBIN_CONNETION=[{"AZURE_AI_INFERENCE_ENDPOINT":"https://models.inference.ai.azure.com","AZURE_AI_INFERENCE_API_KEY":"github_pat_XXXX"},{"AZURE_AI_INFERENCE_ENDPOINT":"https://models.inference.ai.azure.com","AZURE_AI_INFERENCE_API_KEY":"XXX"},{"AZURE_AI_INFERENCE_ENDPOINT":"https://models.inference.ai.azure.com","AZURE_AI_INFERENCE_API_KEY":"XXX"},{"AZURE_AI_INFERENCE_ENDPOINT":"https://models.inference.ai.azure.com","AZURE_AI_INFERENCE_API_KEY":"XXX"},{"AZURE_AI_INFERENCE_ENDPOINT":"https://models.inference.ai.azure.com","AZURE_AI_INFERENCE_API_KEY":"XXX" }]

# azure o1 & o3-mini configuration
AZURE_O1_AND_O3_INFERENCE_ENDPOINT="https://XXXX.openai.azure.com/"
AZURE_O1_AND_O3_INFERENCE_CREDENTIAL="XXX"
AZURE_O1_AND_O3_INFERENCE_API_VERSION="2024-12-01-preview"
AZURE_O1_DEPLOYMENT_NAME="o1"
AZURE_O3_MINI_DEPLOYMENT_NAME="o3-mini"

# cache configuration
CACHE_DIR_PATH = "cache"

# concurrent config
CONCURRENT_TASK_SEMAPHORE_COUNT = 48

# Azure OpenAI o1-preview & o1-mini  round robin configuration
AZURE_OPENAI_ROUND_ROBIN_CONNETION=[{"AZURE_OPENAI_ENDPOINT":"https://XXX.openai.azure.com/","AZURE_OPENAI_API_KEY":"XX"},{"AZURE_OPENAI_ENDPOINT":"https://XXXX.openai.azure.com/","AZURE_OPENAI_API_KEY":"XXXX"},{"AZURE_OPENAI_ENDPOINT":"https://XXX.openai.azure.com/","AZURE_OPENAI_API_KEY":"XXX"},{"AZURE_OPENAI_ENDPOINT":"https://XXXX.openai.azure.com/","AZURE_OPENAI_API_KEY":"XXXX"}]
AZURE_OPENAI_ROUND_ROBIN_API_VERSION="2024-12-01-preview"
AZURE_OPENAI_ROUND_ROBIN_DEPLOYMENT_NAME="o1-preview"
